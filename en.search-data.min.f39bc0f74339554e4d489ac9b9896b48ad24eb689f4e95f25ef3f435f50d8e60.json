[{"id":0,"href":"/docs/Netty/component/","title":"Netty 中的核心组件","section":"Netty","content":" Netty 中的核心组件 # ByteBuf # Channel # EmbeddedChannel LocalServerChannel NioDatagramChannel NioSctpChannel NioSocketChannel EventLoop # ChannelHandler 和 ChannelPipeline # "},{"id":1,"href":"/docs/system-programming/network-byte-order/","title":"网络字节序","section":"Linux 网络编程","content":" 网络字节序 # 大小端 # 不同架构的 CPU 中，4 字节整数 1 在内存中存储的方式是不同的。\n大端序（big endian）：最高位有效字节存储在低内存地址，而最低位有效字节存储在高内存地址。 小端序（little endian）：最高位有效字节存储在高内存地址，而最低位有效字节存储在低内存地址。 对于一个 4 字节整数 0x01020304，大小端序存储方式分别如下：\n地址: 0 1 2 3 （大端序保存） 01 02 03 04 地址: 0 1 2 3 （小端序保存） 04 03 02 01 可以使用下边的方法判断机器的字节序：\n通过 endian.h 提供的 BYTE_ORDER 宏。 #include \u0026lt;endian.h\u0026gt; bool big_endian() { return BYTE_ORDER == BIG_ENDIAN; } bool little_endian() { return BYTE_ORDER == LITTLE_ENDIAN; } 将 uint16_t 类型的数字转为 char *，通过高字节和低字节进行判断。 bool big_endian() { uint16_t val = 0x0102; return ((char *) (\u0026amp;val))[0] == 0x01; } bool little_endian() { uint16_t val = 0x0102; return ((char *) (\u0026amp;val))[1] == 0x01; } 和方法 2 类似，利用的是 union 相同的内存位置存储不同的数据类型。 union endian { uint16_t val; char bytes[2]; }; bool big_endian() { union endian en{}; en.val = 0x0102; return en.bytes[0] == 0x01; } bool little_endian() { union endian en{}; en.val = 0x0102; return en.bytes[0] == 0x02; } 为什么有两种不同的字节序？\n大端序是人类最熟悉的读写方法，从左向右处理。\n小端序更利于计算机处理，因为计算都是从低位开始的，先处理低位字节，效率比较高。\n网络字节序 # 如果通信双方采用不同的架构，收发数据后进行解析时会发生问题。如：大端序机器 A 发送 0x01020304 到小端序机器 B 时，B 以小端序方式解析该数字为 0x04030201。\n为解决上边问题，网络传输数据时，通信双方需要约定统一方式，把此约定叫做网络字节序（network byte order）。\n网络字节序规定使用大端序，大多数网络协议（例如 TCP/IP 协议族）规定了网络字节序采用大端序。\n因此，小端序发送数据时，需要先转为大端序。\n字节序转换 # 下边函数用于字节序的相互转换：\n#include \u0026lt;arpa/inet.h\u0026gt; uint32_t htonl(uint32_t hostlong); uint16_t htons(uint16_t hostshort); uint32_t ntohl(uint32_t netlong); uint16_t ntohs(uint16_t netshort); 函数名中的 h 表示主机（host）字节序，n 表示网络（network）字节序；s 表示 short 类型，l 表示 long 类型。\n"},{"id":2,"href":"/docs/Netty/ByteBuf/","title":"ByteBuf","section":"Netty","content":" ByteBuf # 基本结构 # +-------------------+------------------+------------------+-------------+ | discardable bytes | readable bytes | writable bytes | ... | | | (CONTENT) | | | +-------------------+------------------+------------------+-------------+ | | | | | 0 \u0026lt;= readerIndex \u0026lt;= writerIndex \u0026lt;= capacity maxCapacity 相关操作 # 有些方法会返回 this，以支持链式调用。\n容量 # ByteBuf buf = ByteBufAllocator.DEFAULT.buffer(); int capacity = buf.capacity();// 获取当前的容量 int maxCapacity = buf.maxCapacity();// 支持的最大容量，通常为 Integer.MAX_VALUE int readableBytes = buf.readableBytes();// writerIndex - readerIndex int writableBytes = buf.writableBytes();// capacity - writerIndex boolean readable; readable = buf.isReadable(8);// isReadable(int), writerIndex - readerIndex \u0026gt;= size readable = buf.isReadable();// 相当于 isReadable(1) boolean writable; writable = buf.isWritable(8); // isWritable(int), capacity - writerIndex \u0026gt;= size writable = buf.isWritable(); // 相当于 isWritable(1) int maxWritableBytes = buf.maxWritableBytes();// maxCapacity - writerIndex 读写 # readType() 用于读取 Type 类型的数据，writeType() 用于写入 Type 类型的值。\n与 readType() 类似的有 getType()，与 writeType() 类似的有 setType()。唯一的区别是 getType() 和 setType() 不会改变读写指针，而 readType() 和 writeType()会改变读写指针。\nByteBuf buf = ByteBufAllocator.DEFAULT.buffer(); boolean booleanV = true; buf.writeBoolean(booleanV); // 8-bit buf.readBoolean(); byte byteV = 0; buf.writeByte(byteV); // 8-bit buf.readByte(); short shortV = 0; buf.writeShort(shortV); // 16-bit buf.readShort(); int mediumV = 0; buf.writeMedium(mediumV); // 24-bit buf.readMedium(); int intV = 0; buf.writeInt(intV); // 32-bit buf.readInt(); long longV = 0; buf.writeLong(longV); // 64-bit buf.readLong(); char charV = 0; buf.writeChar(charV); // 8-bit buf.readChar(); float floatV = 0; buf.writeFloat(floatV); // 32-bit buf.readFloat(); double doubleV = 0.0; buf.writeDouble(doubleV); // 64-bit buf.readDouble(); byte[] bytes = new byte[]{0, 1, 2, 3, 4, 5, 6}; // 等价于 buf.writeBytes(bytes, 0, bytes.length) buf.writeBytes(bytes); // 从 `bytes` 的 `srcIndex` 处读取`length` 字节，写入到 `buf` buf.writeBytes(bytes, 1, 3); byte[] bytesDestination = new byte[8]; buf.readBytes(bytesDestination); ByteBuf originalBuf = ByteBufAllocator.DEFAULT.buffer(); // 等价于 buf.writeBytes(originalBuf, originalBuf.readableBytes()) buf.writeBytes(originalBuf); // 等价于 buf.writeBytes(originalBuf, originalBuf.readerIndex(), 3); buf.writeBytes(originalBuf, 3); // 从 `originalBuf` 的 `srcIndex` 处读取`length` 字节，写入到 `buf` buf.writeBytes(originalBuf, 1, 3); ByteBuf bufDestination = ByteBufAllocator.DEFAULT.buffer(); buf.readBytes(bufDestination); 派生 # duplicate() slice() slice(int, int) readSlice(int) retainedDuplicate() retainedSlice() retainedSlice(int, int) readRetainedSlice(int) 读写指针 # ByteBuf buf = ByteBufAllocator.DEFAULT.buffer(); int readerIndex = buf.readerIndex(); // 获取当前读指针位置 int writerIndex = buf.writerIndex(); // 获取当前写指针位置 buf.readerIndex(0); // readerIndex(int)，设置读指针位置 buf.writerIndex(0); // writerIndex(int)，设置写指针位置 buf.markReaderIndex(); // 记录当前读指针位置 // do something buf.resetReaderIndex(); // 恢复到之前 mark 的读指针位置 buf.markWriterIndex(); // 记录当前写指针位置 // do something buf.resetWriterIndex(); // 恢复到之前 mark 的写指针位置 丢弃字节 # ByteBuf buf = ByteBufAllocator.DEFAULT.buffer(); System.out.println(buf); // PooledUnsafeDirectByteBuf(ridx: 0, widx: 0, cap: 256) for (int i = 0; i \u0026lt; 16; i++) { buf.writeInt(i); } for (int i = 0; i \u0026lt; 8; i++) { buf.readInt(); } System.out.println(buf); // PooledUnsafeDirectByteBuf(ridx: 32, widx: 64, cap: 256) buf.discardReadBytes(); System.out.println(buf); // PooledUnsafeDirectByteBuf(ridx: 0, widx: 32, cap: 256) 调用 discardReadBytes() 丢弃 0 ~ readerIndex 之间的字节。将 readerIndex 和 writerIndex 之间的字节移动到第 0 个索引，并将 readerIndex 和 writerIndex 分别设置为 0 和 oldWriterIndex - oldReaderIndex。\nBEFORE discardReadBytes() +-------------------+------------------+------------------+ | discardable bytes | readable bytes | writable bytes | +-------------------+------------------+------------------+ | | | | 0 \u0026lt;= readerIndex \u0026lt;= writerIndex \u0026lt;= capacity AFTER discardReadBytes() +------------------+--------------------------------------+ | readable bytes | writable bytes (got more space) | +------------------+--------------------------------------+ | | | readerIndex (0) \u0026lt;= writerIndex (decreased) \u0026lt;= capacity 也可以调用 clear() 方法，当调用后会直接将 readerIndex 和 writerIndex 同时设为 0。\nBEFORE clear() +-------------------+------------------+------------------+ | discardable bytes | readable bytes | writable bytes | +-------------------+------------------+------------------+ | | | | 0 \u0026lt;= readerIndex \u0026lt;= writerIndex \u0026lt;= capacity AFTER clear() +---------------------------------------------------------+ | writable bytes (got more space) | +---------------------------------------------------------+ | | 0 = readerIndex = writerIndex \u0026lt;= capacity ByteBufHolder # ByteBufHolder 可用于封装 ByteBuf 数据，可用于存储消息对象。\n分类 # Heap 和 Direct # Heap 底层使用 byte[]。\nDirect 底层使用 java.nio.ByteBuffer。\nPooled 和 Unpooled # 池化和非池化的区别，Netty 维护者（Norman Maurer）的 回答：\nThe difference is that with unpooled Netty will allocate a new buffer everytime you call ByteBufAllocator.buffer which comes with some overhead, especially with direct buffers. When you use pooled Netty will try to pool the buffers and so minimize the overhead of allocation and releasing of buffers. Safe 和 Unsafe # Unsafe 的 ByteBuf 会使用 sun.misc.Unsafe 直接申请内存。\nAbstractByteBuf # AbstractByteBuf 是 buffer 的基本骨架，提供了基本的操作。\n类中共有 4 个 *Index 和 1 个指定最大容量的字段 maxCapacity。\nint readerIndex; int writerIndex; private int markedReaderIndex; private int markedWriterIndex; private int maxCapacity; readerIndex 和 writerIndex 分别指定当前读入和写入的位置，当调用 read* 和 write* 方法时会修改位置，而 get* 和 set* 方法不会。\nmarkedReaderIndex 和 markedWriterIndex 用于标注当前的位置，默认值为 0。调用 markReaderIndex() 和 markWriterIndex() 方法用于标记当前读入和写入的位置。\n当调用 resetReaderIndex() 和 resetWriterIndex() 方法时，会将当前的 readerIndex 和 writerIndex 设置为 markedReaderIndex 和 markedWriterIndex。\npublic ByteBuf resetReaderIndex() { readerIndex(markedReaderIndex); return this; } public ByteBuf resetWriterIndex() { writerIndex(markedWriterIndex); return this; } 可读可写只需要判断 writerIndex 和 readerIndex 的相对位置即可。\npublic boolean isReadable() { return writerIndex \u0026gt; readerIndex; } public boolean isWritable() { return capacity() \u0026gt; writerIndex; } public int readableBytes() { return writerIndex - readerIndex; } public int writableBytes() { return capacity() - writerIndex; } ByteBufAllocator # 继承关系 # 默认 Allocator # ByteBufAllocator 类中的 DEFAULT 指定 Netty 使用的 Allocator。\npublic interface ByteBufAllocator { ByteBufAllocator DEFAULT = ByteBufUtil.DEFAULT_ALLOCATOR; } ByteBufUtil 类通过获取系统变量确认默认的 ByteBufAllocator，默认使用 PooledByteBufAllocator 类。\n可以通过 java -Dio.netty.allocator.type: {unpooled|pooled} 指定。\npublic final class ByteBufUtil { ... static final ByteBufAllocator DEFAULT_ALLOCATOR; static { String allocType = SystemPropertyUtil.get( \u0026#34;io.netty.allocator.type\u0026#34;, PlatformDependent.isAndroid() ? \u0026#34;unpooled\u0026#34; : \u0026#34;pooled\u0026#34;); allocType = allocType.toLowerCase(Locale.US).trim(); ByteBufAllocator alloc; if (\u0026#34;unpooled\u0026#34;.equals(allocType)) { alloc = UnpooledByteBufAllocator.DEFAULT; logger.debug(\u0026#34;-Dio.netty.allocator.type: {}\u0026#34;, allocType); } else if (\u0026#34;pooled\u0026#34;.equals(allocType)) { alloc = PooledByteBufAllocator.DEFAULT; logger.debug(\u0026#34;-Dio.netty.allocator.type: {}\u0026#34;, allocType); } else { alloc = PooledByteBufAllocator.DEFAULT; logger.debug(\u0026#34;-Dio.netty.allocator.type: pooled (unknown: {})\u0026#34;, allocType); } DEFAULT_ALLOCATOR = alloc; } ... } 内存回收 # Netty 使用引用计数方法对 ByteBuf 进行回收。实现 ReferenceCounted 的实例开始时的引用计数为 1，只要引用计数大于 0，就能保证对象不会被释放。当引用计数减少到 0 时，该实例就会被释放。\nUnpooledHeapByteBuf 使用的是 JVM 内存，只需等 GC 回收即可。 UnpooledDirectByteBuf 使用了直接内存， 扩容逻辑 # 当向 ByteBuf 写入数据，而容量不足时（writerIndex \u0026gt; capacity()），会自动进行扩容。\n每次调用 write*() 时，方法内部会调用 ensureWritable0() 确保有足够的空间。\n调用 ensureWritable0() 时会传入 minWritableBytes 参数，确保有足够的字节数，调用 writeLong() 时，会被设为 8。\npublic abstract class AbstractByteBuf extends ByteBuf { ... public ByteBuf writeInt(int value) { ensureWritable0(4); _setInt(writerIndex, value); writerIndex += 4; return this; } ... } ensureWritable0 # ensureWritable0() 进行扩容的代码如下：\n获取当前的写入索引（writerIndex），并计算目标容量（targetCapacity），即当前写入索引加上要写入的最小字节数。 使用非短路逻辑与运算符（\u0026amp;）来检查目标容量是否在合理范围内。这里选择使用非短路逻辑与运算符是为了减少分支，因为该代码段通常是一个热点路径，并且目标容量很少会溢出。如果目标容量在合理范围内，则表示缓冲区已经有足够的空间，直接返回。 如果启用了边界检查（checkBounds），并且目标容量小于 0 或者大于最大容量（maxCapacity），则抛出索引越界异常。 如果目标容量不在合理范围内，需要增加缓冲区的容量。首先，调用 maxFastWritableBytes() 方法获取一个快速可写入的字节数，然后根据这个字节数和传入的最小可写入字节数来计算新的容量。如果快速可写入字节数大于等于传入的最小可写入字节数，则新容量直接设定为当前写入索引加上快速可写入字节数，否则通过调用 calculateNewCapacity 方法来计算新容量。 通过调用 capacity(newCapacity) 方法来调整缓冲区的容量，确保其足够可写入。 public abstract class AbstractByteBuf extends ByteBuf { ... final void ensureWritable0(int minWritableBytes) { final int writerIndex = writerIndex(); final int targetCapacity = writerIndex + minWritableBytes; // using non-short-circuit \u0026amp; to reduce branching - this is a hot path and targetCapacity should rarely overflow if (targetCapacity \u0026gt;= 0 \u0026amp; targetCapacity \u0026lt;= capacity()) { ensureAccessible(); return; } if (checkBounds \u0026amp;\u0026amp; (targetCapacity \u0026lt; 0 || targetCapacity \u0026gt; maxCapacity)) { ensureAccessible(); throw new IndexOutOfBoundsException(String.format( \u0026#34;writerIndex(%d) + minWritableBytes(%d) exceeds maxCapacity(%d): %s\u0026#34;, writerIndex, minWritableBytes, maxCapacity, this)); } // Normalize the target capacity to the power of 2. final int fastWritable = maxFastWritableBytes(); int newCapacity = fastWritable \u0026gt;= minWritableBytes ? writerIndex + fastWritable : alloc().calculateNewCapacity(targetCapacity, maxCapacity); // Adjust to the new capacity. capacity(newCapacity); } ... } calculateNewCapacity # calculateNewCapacity 计算新的容量时的思路总结如下：\nminNewCapacity 参数为最小申请的容量，检查传入的 minNewCapacity 参数是否为非负数，如果不是，则抛出异常。 检查 minNewCapacity 是否超过了最大容量 maxCapacity，如果超过了，则抛出异常。 设定一个阈值 threshold，表示页面大小为 4 MiB。 minNewCapacity 等于阈值 threshold，则直接返回阈值，不再需要进行容量的调整。 minNewCapacity 大于阈值 threshold，则按照阈值为单位递增容量。在这种情况下，新容量的计算方式不再是简单的翻倍增加，而是以阈值为单位递增。 minNewCapacity 小于等于阈值 threshold 时，则将 minNewCapacity 设置为大于等于 64 的最接近的 2 的幂，以确保足够的容量同时尽量减小内存的浪费。 返回新容量和最大容量中较小的一个值，以确保新容量不会超过最大容量限制。 public abstract class AbstractByteBufAllocator implements ByteBufAllocator { ... public int calculateNewCapacity(int minNewCapacity, int maxCapacity) { checkPositiveOrZero(minNewCapacity, \u0026#34;minNewCapacity\u0026#34;); if (minNewCapacity \u0026gt; maxCapacity) { throw new IllegalArgumentException(String.format( \u0026#34;minNewCapacity: %d (expected: not greater than maxCapacity(%d)\u0026#34;, minNewCapacity, maxCapacity)); } final int threshold = CALCULATE_THRESHOLD; // 4 MiB page if (minNewCapacity == threshold) { return threshold; } // If over threshold, do not double but just increase by threshold. if (minNewCapacity \u0026gt; threshold) { int newCapacity = minNewCapacity / threshold * threshold; if (newCapacity \u0026gt; maxCapacity - threshold) { newCapacity = maxCapacity; } else { newCapacity += threshold; } return newCapacity; } // 64 \u0026lt;= newCapacity is a power of 2 \u0026lt;= threshold final int newCapacity = MathUtil.findNextPositivePowerOfTwo(Math.max(minNewCapacity, 64)); return Math.min(newCapacity, maxCapacity); } ... } "},{"id":3,"href":"/docs/system-programming/address-families/","title":"网络地址族","section":"Linux 网络编程","content":" 网络地址族 # 网络地址 # 网络地址分为 IPv4 和 IPv6，分别使用 sockaddr_in 和 sockaddr_in6 结构体表示。\nsockaddr_in # struct sockaddr_in { sa_family_t sin_family; /* address family: AF_INET */ in_port_t sin_port; /* port in network byte order */ struct in_addr sin_addr; /* internet address */ /* Pad to size of `struct sockaddr\u0026#39;. */ unsigned char sin_zero[8]; }; /* Internet address */ struct in_addr { uint32_t s_addr; /* address in network byte order */ }; sin_family：在 IPv4 中设为 AF_INET。 sin_port：网络字节序保存的端口（0～65535）。 sin_addr：网络字节序保存的 32 位 IP 地址信息。 sin_zero：使 sockaddr_in 和 sockaddr 结构体大小保持一致而插入的填充位，需手动设为 0。 https://man7.org/linux/man-pages/man7/ip.7.html\nsockaddr_in6 # struct sockaddr_in6 { sa_family_t sin6_family; /* AF_INET6 */ in_port_t sin6_port; /* port number */ uint32_t sin6_flowinfo; /* IPv6 flow information */ struct in6_addr sin6_addr; /* IPv6 address */ uint32_t sin6_scope_id; /* Scope ID (new in Linux 2.4) */ }; struct in6_addr { unsigned char s6_addr[16]; /* IPv6 address */ }; sin6_family：在 IPv6 中总设为 AF_INET6。 sin6_port：网络字节序保存的端口（0～65535）。 sin6_flowinfo：IPv6 流信息（不广泛使用）。 sin6_addr：表示 IPv6 地址的结构体，定义为 struct in6_addr。 sin6_scope_id：范围标识符（用于链路本地和站点本地地址）。 https://man7.org/linux/man-pages/man7/ipv6.7.html\nin_addr 和 in6_addr # in_addr # 我们比较熟悉的 IPv4 地址表示方法为点分十进制表示法，如：201.123.235.213。\n而 in_addr 结构体使用 uint32_t 保存 IPv4 地址，我们需要将字符串形式的 IPv4 地址转为 32 位整数表示。\ninet_addr 函数可用于转换，该函数在转换的同时会进行网络字节序的转换。\ninet_ntoa 函数则相反，将 in_addr 结构体转为字符串。\n注意：inet_ntoa 函数返回的是一个指向静态缓冲区的指针，最好将返回值拷贝到其它地方，以免被覆盖。\n#include \u0026lt;arpa/inet.h\u0026gt; in_addr_t inet_addr(const char *cp); char *inet_ntoa(struct in_addr in); 也可以使用 inet_aton 函数，该函数将结果直接保存到传入的 inp 结构体中。\n#include \u0026lt;arpa/inet.h\u0026gt; int inet_aton(const char *cp, struct in_addr *inp); in6_addr # sockaddr_in5 结构体中的 in6_addr 结构体包含一个 unsigned char 类型的成员 s6_addr，用于存储 128 位的 IPv6 地址。\ninet_pton 函数中的 af 参数必须为 AF_INET 和 AF_INET6，分别处理 IPv4 和 IPv6 协议。\n#include \u0026lt;arpa/inet.h\u0026gt; int inet_pton(int af, const char *restrict src, void *restrict dst); const char *inet_ntop(int af, const void *restrict src, char dst[restrict .size], socklen_t size); 网络地址初始化 # IPv4 # #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; int main() { const char *address = \u0026#34;211.123.211.168\u0026#34;; // IP 地址 const char *port = \u0026#34;7890\u0026#34;; // 端口 struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(addr)); addr.sin_family = AF_INET; // IPv4; addr.sin_addr.s_addr = inet_addr(address); // 设置 IP 地址 addr.sin_port = htons(atoi(port)); // 以网络字节序设置端口 } IPv6 # #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; int main() { const char *address = \u0026#34;2001:0db8:85a3:0000:0000:8a2e:0370:7334\u0026#34;; // IPv6 地址 const char *port = \u0026#34;7890\u0026#34;; // 端口 struct sockaddr_in6 addr; memset(\u0026amp;addr, 0, sizeof(addr)); addr.sin6_family = AF_INET6; // IPv6 inet_pton(AF_INET6, address, \u0026amp;addr.sin6_addr); // 设置 IP 地址 addr.sin6_port = htons(atoi(port)); // 以网络字节序设置端口 } 思考 # 为什么 sockaddr_in 和 sockaddr_in6 分别表示 IPv4 和 IPv6 协议，还要额外使用 sa_family_t 指定协议版本呢？\n因为 connect、bind、和 accept 函数第二个参数都接收 sockaddr 结构体。因此，需要使用 sa_family 用于区分不同版本的协议。通过使用通用的 sockaddr 结构体，该函数不仅可以处理 IPv4 和 IPv6 协议，还可处理其它协议。这样，就不需要为每种协议都提供对应的函数。\nstruct sockaddr { sa_family_t sa_family; /* Address family */ char sa_data[]; /* Socket address */ }; "},{"id":4,"href":"/docs/Netty/Pooled-and-Unpooled-ByteBuf/","title":"Pooled 和 Unpooled 的 ByteBuf","section":"Netty","content":" Pooled 和 Unpooled 的 ByteBuf # # new byte[initialCapacity];\n"},{"id":5,"href":"/docs/system-programming/block-and-nonblock-io/","title":"（非）阻塞 I/O","section":"Linux 网络编程","content":" （非）阻塞 I/O 和 epoll（翻译） # 原文：https://eklitzke.org/blocking-io-nonblocking-io-and-epoll\n在这篇文章中，我想解释使用非阻塞 I/O 时会发生什么。我特别想说明的是:\n使用 fcntl 函数设置文件描述符的 O_NONBLOCK 时的语义。 非阻塞（nonblocking） I/O 与异步（asynchronous） I/O 的区别。 为什么非阻塞 I/O 经常与诸如 select、epoll 和 kqueue 等 I/O 多路复用器一起使用。 非阻塞模式如何与 epoll 中的边缘触发轮询交互。 阻塞模式 # 默认情况下，Unix 系统所有的文件描述符都以“阻塞模式”启动。这意味着像 read、write 或 connect 这样的 I/O 系统调用可能会阻塞。一个很容易理解的方法是当你从一个普通的基于 TTY 的程序中的 stdin 读取数据时会发生什么。如果你在 stdin上调用 read，那么你的程序将会阻塞，直到数据实际上可用，比如当用户实际上在键盘上键入字符时。具体来说，内核会将进程置于“睡眠”状态，直到 stdin 上的数据可用。其他类型的文件描述符也是如此。例如，如果你尝试从 TCP 套接字中读取数据，那么 read 调用将会阻塞，直到连接的另一端实际上发送数据。\n阻塞对于应该并发运行的程序来说是一个问题，因为被阻塞的进程会被挂起。解决这个问题有两种不同但互补的方式：\n非阻塞模式。 I/O 多路复用系统调用，例如 select 和 epoll。 这两种解决方案经常一起使用，但它们是解决这个问题的独立策略，通常两者都会被使用。接下来我们将会看到它们之间的区别以及为什么它们通常都会被同时使用。\n非阻塞模式 # 通过 fcntl 函数在文件描述符的标志集中添加 O_NONBLOCK，可以将文件描述符设置为“非阻塞模式”：\n/* set O_NONBLOCK on fd */ int flags = fcntl(fd, F_GETFL, 0); fcntl(fd, F_SETFL, flags | O_NONBLOCK); 从这一点开始，文件描述符被视为非阻塞的。当发生这种情况时，像 read 和 write 这样的 I/O 系统调用将返回 -1，并且 errno 将被设置为 EWOULDBLOCK。\n这很有趣，但单独使用实际上并不是那么有用。仅仅使用这种基本方法是没有有效方式同时在多个文件描述符上进行 I/O 的。例如，假设我们有两个文件描述符，并希望同时读取它们。这可以通过循环检查每个文件描述符是否有数据，然后在再次检查之前短暂休眠来实现：\nstruct timespec sleep_interval{.tv_sec = 0, .tv_nsec = 1000}; ssize_t nbytes; for (;;) { /* try fd1 */ if ((nbytes = read(fd1, buf, sizeof(buf))) \u0026lt; 0) { if (errno != EWOULDBLOCK) { perror(\u0026#34;read/fd1\u0026#34;); } } else { handle_data(buf, nbytes); } /* try fd2 */ if ((nbytes = read(fd2, buf, sizeof(buf))) \u0026lt; 0) { if (errno != EWOULDBLOCK) { perror(\u0026#34;read/fd2\u0026#34;); } } else { handle_data(buf, nbytes); } /* sleep for a bit; real version needs error checking! */ nanosleep(sleep_interval, NULL); } 这种方法确实有效，但存在很多缺点：\n当数据传入速度很慢时，程序会频繁而不必要地唤醒，这会浪费 CPU 资源。 当数据到达时，如果程序正在睡眠，可能不会立即读取数据，因此程序的延迟会很高。 使用这种模式处理大量文件描述符会变得繁琐。 为了解决这些问题，我们需要 I/O 多路复用。\nI/O 多路复用（select, epoll, kqueue） # 有几种I/O多路复用系统调用。例如，POSIX 定义的 select，Linux 上的 epoll 系列，以及 BSD 上的 kqueue 系列都是 I/O 多路复用的例子。它们在基本原理上都是相同的：它们让内核知道一组文件描述符上感兴趣的事件（通常是读事件和写事件），然后它们会阻塞，直到发生感兴趣的事件。例如，你可以告诉内核你只对文件描述符 X 上的读事件感兴趣，对文件描述符 Y 上的读和写事件都感兴趣，以及对文件描述符 Z 上的写事件感兴趣。\n这些 I/O 多路复用系统调用通常不关心文件描述符是处于阻塞模式还是非阻塞模式。你可以将所有的文件描述符都保留在阻塞模式下，它们仍然可以与 select 或 epoll 一起正常工作。如果你只对 select 或 epoll 返回的文件描述符调用 read 和 write，那么即使这些文件描述符处于阻塞模式，这些调用也不会阻塞。但有一个重要的例外，文件描述符的阻塞或非阻塞状态对于边缘触发轮询是重要的，下面会进一步解释。\n并发的多路复用方法是我所谓的“异步 I/O”。有时人们也会将这种方法称为“非阻塞 I/O”，我认为这是对系统编程层面中“非阻塞”含义的混淆。我建议将术语“非阻塞”保留用于指代文件描述符是否实际处于非阻塞模式。\nO_NONBLOCK 与 I/O 多路复用的交互方式 # 假设我们正在使用带有阻塞文件描述符的 select 编写一个简单的套接字服务器。为简单起见，在此示例中，我们只有要从中读取的文件描述符，这些文件描述符存储在 read_fds 中。事件循环的核心部分将调用 select，然后针对每个具有数据的文件描述符调用一次 read：\nssize_t nbytes; for (;;) { /* select call happens here */ if (select(FD_SETSIZE, \u0026amp;read_fds, NULL, NULL, NULL) \u0026lt; 0) { perror(\u0026#34;select\u0026#34;); exit(EXIT_FAILURE); } for (int i = 0; i \u0026lt; FD_SETSIZE; i++) { if (FD_ISSET(i, \u0026amp;read_fds)) { /* read call happens here */ if ((nbytes = read(i, buf, sizeof(buf))) \u0026gt;= 0) { handle_read(nbytes, buf); } else { /* real version needs to handle EINTR correctly */ perror(\u0026#34;read\u0026#34;); exit(EXIT_FAILURE); } } } } 这样做是有效的，而且完全没问题。但是，如果 buf 很小，而且有大量数据同时传输会发生什么？具体来说，假设 buf 是一个 1024 字节的缓冲区，但一次传输了 64KB 的数据。为了处理这个请求，我们将调用 select，然后调用 64 次 read。总共需要 128 次系统调用，这是相当多的。\n如果缓冲区大小太小，就必须调用很多次 read，这是无法避免的。但也许我们可以减少调用 select 的次数？在这个例子中，理想情况下我们只会调用一次 select。\n事实上，这是可能的，而且可以通过将文件描述符设置为非阻塞模式来实现。基本思想是你可以在一个循环中不断调用 read，直到它返回 EWOULDBLOCK 为止。实现如下所示：\nssize_t nbytes; for (;;) { /* select call happens here */ if (select(FD_SETSIZE, \u0026amp;read_fds, NULL, NULL, NULL) \u0026lt; 0) { perror(\u0026#34;select\u0026#34;); exit(EXIT_FAILURE); } for (int i = 0; i \u0026lt; FD_SETSIZE; i++) { if (FD_ISSET(i, \u0026amp;read_fds)) { /* NEW: loop until EWOULDBLOCK is encountered */ for (;;) { /* read call happens here */ nbytes = read(i, buf, sizeof(buf)); if (nbytes \u0026gt;= 0) { handle_read(nbytes, buf); } else { if (errno != EWOULDBLOCK) { /* real version needs to handle EINTR correctly */ perror(\u0026#34;read\u0026#34;); exit(EXIT_FAILURE); } break; } } } } } 在这个例子中（1024 字节缓冲区，传入 64KB 的数据），我们将进行 66 次系统调用：select 将被调用一次，read 将被调用 64 次而不会出错，read 将被调用并返回 EWOULDBLOCK 一次。这要比之前的例子好得多！这几乎是前一个例子的一半，这将显著提高性能和可伸缩性。\n这种方法的缺点是由于新的循环，至少会有一次额外的 read 调用，因为它被调用直到返回 EWOULDBLOCK。假设通常情况下读取缓冲区足够大，可以在一个 read 调用中读取所有传入的数据。那么在循环中，通常情况下会有三次系统调用而不是只有两次：select 等待数据，read 实际读取数据，然后再次调用 read 以获取 EWOULDBLOCK。\n边缘触发轮询（Edge-Triggered Polling） # 边缘触发轮询是非阻塞 I/O 的另一个重要用途，特别是在 epoll 系统调用中。这个系统调用有两种模式：水平触发和边缘触发。水平触发是一种更简单的编程模型，类似于经典的 select 系统调用。为了解释它们之间的区别，我们需要了解 epoll 在内核中的工作方式。\n假设你告诉内核你有兴趣使用 epoll 来监视某个文件描述符上的读事件。内核为每个文件描述符维护这些兴趣的列表。当数据到达文件描述符时，内核遍历兴趣列表，并唤醒每个在 epoll_wait 中被阻塞的进程，其中包含了该文件描述符在事件列表中。\n上述内容无论 epoll 处于哪种触发模式都会发生。水平触发和边缘触发轮询的区别在于调用 epoll_wait 时内核的行为。在水平触发模式中，内核将遍历兴趣列表中的每个文件描述符，以查看它是否已满足兴趣条件。例如，如果你在文件描述符 8 上注册了一个读事件，调用 epoll_wait 时内核将首先检查：文件描述符 8 是否已经有数据准备好读取？如果任何文件描述符匹配兴趣条件，则 epoll_wait 可以立即返回而不阻塞。\n相比之下，在边缘触发模式中，内核跳过这个检查，并在调用 epoll_wait 时立即将进程置于睡眠状态。这使得程序员必须完全负责，需要完全读取和写入每个文件描述符的所有数据，然后才能等待。\n边缘触发模式使得 epoll 成为时间复杂度为 O(1) 的 I/O 多路复用器：epoll_wait 调用会立即挂起，由于事先为每个文件描述符维护了一个列表，当新数据到达时，内核会在 O(1) 时间知道必须要唤醒的进程。\n以下是边缘触发和水平触发模式之间差异的更详细示例。假设你的读取缓冲区是 100 字节，而文件描述符上传入了 200 字节的数据。然后假设你只调用了一次 read，然后再次调用了 epoll_wait。仍然有 100 字节的数据准备好读取。在水平触发模式中，内核会注意到这一点，并通知进程应该再次调用 read。相比之下，在边缘触发模式中，内核将立即进入睡眠状态。如果另一侧正在期待响应（例如，发送的数据是某种 RPC），那么两侧将会“死锁”，因为服务器将等待客户端发送更多数据，但客户端将等待服务器发送响应。\n要使用边缘触发轮询，你必须将文件描述符设置为非阻塞模式。然后你必须每次调用 read 或 write 直到它们返回 EWOULDBLOCK 为止。如果你未能满足这些条件，你将错过内核的通知。但这样做有一个很大的好处：每次调用 epoll_wait 都会更有效率，这对于具有极高并发性的程序来说非常重要。如果你想了解更多细节，我强烈建议你阅读 epoll(7) 手册页。\n"},{"id":6,"href":"/docs/system-programming/multiplexing/","title":"I/O 多路复用","section":"Linux 网络编程","content":" I/O 多路复用 # I/O 复用可以使程序同时监听多个文件描述符。\nselect # select 函数允许程序监视多个文件描述符，直到一个或多个文件描述符“准备好”进行某类 I/O 操作。\nselect 成功时返回就绪文件描述符的总数，如果超时时间内没有任何文件描述符就绪，则返回 0。失败时返回 -1 并设置 errno。\n#include \u0026lt;sys/select.h\u0026gt; int select(int nfds, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval * timeout); 函数参数 # nfds：指定被监听的文件描述符的总数。这个参数应该被设置为三个集合中编号最高的文件描述符，再加 1，因为文件描述符是从 0 开始的。\nreadfds、writefds 和 exceptfds：分别指向可读、可写和异常事件对应的文件描述符集合。如果没有文件描述符要监听，则可以将对应的 fd_set 参数设为 NULL。\nfd_set 结构体仅包含一个数组，数组每一位标记一个文件描述符，最大容纳长度由 FD_SETSIZE 指定。\n/* fd_set for select and pselect. */ typedef struct { /* XPG4.2 requires this member name. Otherwise avoid the name from the global namespace. */ #ifdef __USE_XOPEN __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS]; # define __FDS_BITS(set) ((set)-\u0026gt;fds_bits) #else __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS]; # define __FDS_BITS(set) ((set)-\u0026gt;__fds_bits) #endif } fd_set; 为方便对此结构体进行操作，提供了以下几个宏函数对其进行操作，宏函数如下：\n为方便展示，对宏函数中的所有参数加上了类型\nFD_SET(fd, fdsetp)：将文件描述符 fd 添加到 fdset 指向的集合中。 FD_CLR(fd, fdsetp)：将文件描述符 fd 从 fdset 指向的集合中移除。 FD_ISSET(fd, fdsetp)：如果文件描述符 fd 是 fdset 指向的集合中的成员，则返回 true。 FD_ZERO(fdsetp)：将 fdset 指向的集合初始化为空。 timeval：用来设置 select 函数的超时时间，采用指针作为参数是因为内核将修改以告诉应用程序 select 等待了多久。\n如果 timeval 结构体中的 tv_sec 和 tv_usec 成员都传递 0，则 select 函数立即返回。如果传递 NULL，则一直阻塞，直到某个文件描述符就绪。\n/* A time value that is accurate to the nearest microsecond but also has a range of years. */ struct timeval { #ifdef __USE_TIME_BITS64 __time64_t tv_sec;\t/* Seconds. */ __suseconds64_t tv_usec;\t/* Microseconds. */ #else __time_t tv_sec;\t/* Seconds. */ __suseconds_t tv_usec;\t/* Microseconds. */ #endif }; 就绪条件 # 在网络编程中，下边情况下 socket 可读：\n在网络编程中，下边情况下 socket 可写：\nselect 函数能处理的异常情况只有一种：socket 上接收到带外数据。\n循环中使用 # 由于这些结构体会在调用中被修改，如果要在循环中重复调用 select 函数，我们必须保证每次都要重新初始化它们。\npoll # poll 函数和 select 函数调用返回值一致。\n#include \u0026lt;poll.h\u0026gt; int poll(struct pollfd *fds, nfds_t nfds, int timeout); 函数参数 # fds：需要 poll 函数检查的文件描述符，该参数为 pollfd 结构体数组。\npollfd 结构体中 fd 指定文件描述符；events 告诉 poll 函数需要监听哪些事件；revents 由内核对其进行修改，以通知应用程序 fd 上实际发生了哪些事件。\n位掩码 events 返回到revents 描述 POLLIN ● ● 可读取非高优先级的数据 POLLRDNORM ● ● 等同于POLLIN POLLRDBAND ● ● 可读取优先级数据（Linux 中不使用） POLLPRI ● ● 可读取高优先级数据 POLLRDHUP ● ● 对端套接字关闭 POLLOUT ● ● 普通数据可写 POLLWRNORM ● ● 等同于POLLOUT POLLWRBAND ● ● 优先级数据可写入 POLLERR ● 有错误发生 POLLHUP ● 出现挂断 POLLNVAL ● 文件描述符未打开 POLLMSG Linux 中不使用 struct pollfd { int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */ }; nfds：用于指定数组 fds 中元素的个数。\n/* Type used for the number of file descriptors. */ typedef unsigned long int nfds_t; timeout：指定 poll 的超时值，单位为毫秒。\n当 timeout 为 -1 时，poll 调用将一直阻塞，直到某个事件发生； 当 timeout 为 0 时，poll 调用将立即返回。 epoll # epoll 是 Linux 特有的 I/O 复用函数。epoll 需要使用额外的文件描述符，标识内核中的这个事件表，需要使用 epoll_create 函数创建，返回文件描述符。\nsize 是想要通过 epoll 来检查的文件描述符个数。该参数并不是一个上限，而是告诉内核应该如何为内部数据结构划分初始大小（从 Linux 2.6.8 版以来，size 参数被忽略不用，因为内核实现做了修改意味着该参数提供的信息已经不再需要了）。\n#include \u0026lt;sys/epoll.h\u0026gt; int epoll_create (int size); int epoll_create1 (int flags); epoll_ctl # epoll_ctl 函数能够修改由文件描述符 epfd 所代表的兴趣列表。\nint epoll_ctl (int epfd, int op, int fd, struct epoll_event *event); epfd 是调用 epoll_create 函数的返回值。\nop 用于操作操作类型。\n/* Valid opcodes ( \u0026#34;op\u0026#34; parameter ) to issue to epoll_ctl(). */ #define EPOLL_CTL_ADD 1\t/* Add a file descriptor to the interface. */ #define EPOLL_CTL_DEL 2\t/* Remove a file descriptor from the interface. */ #define EPOLL_CTL_MOD 3\t/* Change file descriptor epoll_event structure. */ events 是一个位掩码，指定了待检查描述符 fd 上感兴趣的事件集合。\nstruct epoll_event { uint32_t events;\t/* Epoll events */ epoll_data_t data; /* User data variable */ } data 当描述符 fd 就绪时，传递给调用者的信息。\ntypedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; epoll_wait # 返回 epoll 实例中处于就绪态的文件描述符信息。单个 epoll_wait 函数调用能返回多个就绪态文件描述符的信息。\n调用成功后，epoll_wait 函数返回数组 events 元素个数。\nint epoll_wait (int epfd, struct epoll_event *events, int maxevents, int timeout); events 所指向的结构体数组中返回的是有关就绪态文件描述符的信息。\n数组 events 的空间由调用者负责申请，所包含的元素个数由参数 maxevents 指定。\ntimeout：指定 epoll 的超时值，单位为毫秒。\n当 timeout 为 -1 时，epoll 调用将一直阻塞，直到某个事件发生； 当 timeout 为 0 时，epoll 执行一次非阻塞式的检查，看兴趣列表中的文件描述符上产生了哪个事件。 当 timeout 大于 0 时，epoll 阻塞至多 timeout 毫秒，直到文件描述符上有事件发生，或者直到捕获到一个信号为止。 水平触发和边缘触发 # epoll 默认工作模式为水平触发，当往 epoll 内核事件表中注册一个文件描述符上的 EPOLLET 事件时，将以边缘触发模式工作。\nLT 模式（水平）：缓冲区剩余未读尽的数据会导致 epoll_wait 返回。直到新的事件满足才会触发。支持阻塞和非阻塞。\nET 模式（边缘）：缓冲区剩余未读尽的数据不会导致 epoll_wait 返回。必须设置为非阻塞。\n"},{"id":7,"href":"/docs/system-programming/reactor-pattern/","title":"Reactor 模型","section":"Linux 网络编程","content":" Reactor 模型 # Reactor 模型中定义的三种角色：\nReactor：负责监听和分配事件，将I/O事件分派给对应的 Handler。新的事件包含连接建立就绪、读就绪、写就绪等。 Acceptor：处理客户端新连接，并分派请求到处理器链中。 Handler：将自身与事件绑定，执行非阻塞读/写任务，完成channel的读入，完成处理业务逻辑后，负责将结果写出channel。可用资源池来管理。 One Loop Per Thread # 参考文献 # https://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf https://gee.cs.oswego.edu/dl/cpjslides/nio.pdf "},{"id":8,"href":"/docs/system-programming/sockets/","title":"套接字（socket）","section":"Linux 网络编程","content":" 套接字（socket） # 网络编程即编写程序使两台联网的计算机相互交换数据。计算机之间会通过网线、路由器和交换机等设备连接在一起，我们无需直接操控硬件，而使用操作系统提供的套接字（socket）。\n基本函数 # socket # 为了使用套接字，可以使用 socket 函数，创建用于通信的端点（endpoint）。\n#include \u0026lt;sys/socket.h\u0026gt; int socket(int domain, int type, int protocol); 成功时会返回文件描述符，失败时会返回 -1。\nhttps://man7.org/linux/man-pages/man2/socket.2.html\nbind # 当使用 socket 函数创建套接字后，会存在于名称空间（地址族）中，但没有为其分配地址。bind 函数将 addr 指定的地址分配给文件描述符 sockfd 引用的套接字。\n服务器可以不先调用 bind() 而直接调用 listen()，此时会为该 socket 分配一个 INADDR_ANY IP 地址（0.0.0.0）和临时端口（可通过 getsockname() 获取 socket 的地址）。\n#include \u0026lt;sys/socket.h\u0026gt; int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); 成功时返回 0，失败时返回 -1。\nhttps://man7.org/linux/man-pages/man2/bind.2.html\nlisten # listen 函数将文件描述符引用的 socket 标记为被动，该 socket 会被用来接受来自其它主动 socket 的连接。\n#include \u0026lt;sys/socket.h\u0026gt; int listen(int sockfd, int backlog); 成功时返回 0，失败时返回 -1。\nhttps://man7.org/linux/man-pages/man2/listen.2.html\naccept # 执行 accept 函数会创建一个新的 socket，此 socket 会与执行 connect 函数的 socket 进行连接。此函数调用返回值是已连接的 socket 的文件描述符。\n#include \u0026lt;sys/socket.h\u0026gt; int accept(int sockfd, struct sockaddr *_Nullable restrict addr, socklen_t *_Nullable restrict addrlen); 成功时会返回文件描述符，失败时会返回 -1。\nhttps://man7.org/linux/man-pages/man2/accept.2.html\nconnect # connect 函数将文件描述符 sockfd 引用的套接字连接到由 addr 指定的地址。\n#include \u0026lt;sys/socket.h\u0026gt; int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); https://man7.org/linux/man-pages/man2/connect.2.html\n套接字协议 # socket 函数（int socket(int domain, int type, int protocol)）有三个参数用于选择传输协议和方式。\n协议族（domain） # 所有的协议族可以点击 address_families 查看，主要的协议族分类如下：\n协议族 描述 AF_INET 使用 IPv4 地址 AF_INET6 使用 IPv6 地址 AF_UNIX 本地通信，用于同一台机器上的进程间通信 AF_PACKET 原始数据包捕获和注入，需要特殊权限 AF_NETLINK 用于 Linux 内核与用户空间进程之间的通信 AF_常量 和 PF_常量 的区别？\nAF 表示地址族（address family），PF 表示协议族（protocol family）。在一开始的时候，设计人员相信单个协议族可以支持多个地址族。但在实践中，没有哪一个协议族能够支持多个已经被定义的地址族，并且所有既有实现都将 PF_常量 定义成对应的 AF_常量 的同义词。\n数据传输方式（type） # 数据传输类型主要有以下两种：\n面向连接的套接字（SOCK_STREAM）：提供有序的、可靠的、双向的、基于连接的字节流。可以支持带外（ out-of-band）数据传输机制。 面向消息的套接字（SOCK_DGRAM）：支持数据报（无连接、最大长度固定的不可靠消息）。 协议（protocol） # 在给定的协议族中，通常只有一个协议存在以支持特定的套接字类型，在这种情况下，可以将 protocol 指定为 0。\n在大部分情况下，第三个参数传递 0 即可。然而，协议族下可能存在许多协议，在这种情况下，必须使用 protocol 指定一个特定的协议。\n"},{"id":9,"href":"/docs/system-programming/echo-server/","title":"echo 服务器","section":"Linux 网络编程","content":" echo 服务器 # 辅助函数 # panic 函数用于错误处理，当发生错误时，调用 exit 函数直接退出程序。\nvoid panic(const char *format, ...) { va_list args; va_start(args, format); vfprintf(stderr, format, args); va_end(args); exit(EXIT_FAILURE); } readn 和 writen 函数分别用于从 fd 处读和写 n 个字节。\nssize_t nread; while (nleft \u0026gt; 0) { if ((nread = read(fd, buf, nleft)) \u0026lt; 0) { if (nleft == n) { return -1; // error, return -1 } else { break; // error, return amount read so far } while (nleft \u0026gt; 0) { if ((nread = read(fd, buf, nleft)) \u0026lt; 0) { if (nleft == n) { return -1; // error, return -1 } else { } nleft -= nread; buf += nread; } return n - nleft; } ssize_t writen(int fd, const void *buf, size_t n) { size_t nleft = n; ssize_t nwritten; while (nleft \u0026gt; 0) { if ((nwritten = write(fd, buf, nleft)) \u0026lt; 0) { if (nleft == n) { return -1; } else { break; } } else if (nwritten == 0) { break; } nleft -= nwritten; buf += nwritten; } return n - nleft; } read_line 函数用于从用户处读取一行输入。\nchar *read_line() { char *line = NULL; size_t buf_size = 0; ssize_t n_bytes = getline(\u0026amp;line, \u0026amp;buf_size, stdin); if (n_bytes == -1) { fprintf(stderr, \u0026#34;error: reading input\\n\u0026#34;); } if (line[n_bytes - 1] == \u0026#39;\\n\u0026#39;) { line[n_bytes - 1] = \u0026#39;\\0\u0026#39;; } return line; } set_nonblocking 函数用于将 fd 设为非阻塞模式。\nint set_nonblocking(int fd) { int flags = fcntl(fd, F_GETFL, 0); if (flags == -1 || fcntl(fd, F_SETFL, flags | O_NONBLOCK) == -1) { return -1; } return 0; } create_sockaddr_in 函数用于创建 struct sockaddr_in 结构体。\nstruct sockaddr_in *create_sockaddr_in(const char *address, int port) { struct sockaddr_in *addr = malloc(sizeof(struct sockaddr_in)); addr-\u0026gt;sin_family = AF_INET; addr-\u0026gt;sin_addr.s_addr = inet_addr(address); addr-\u0026gt;sin_port = htons(port); return addr; } 对于 socket、bind、listen、accept 和 connect 函数，大部分使用都是一样的，对其进行封装。\nint Socket() { int fd = socket(AF_INET, SOCK_STREAM, 0); if (fd == -1) { panic(\u0026#34;socket() error: %s\\n\u0026#34;, strerror(errno)); } return fd; } void Bind(int socket_fd, struct sockaddr_in *addr) { int ret = bind(socket_fd, (struct sockaddr *)(addr), sizeof(*addr)); if (ret == -1) { panic(\u0026#34;bind() error: %s\\n\u0026#34;, strerror(errno)); } } void Listen(int socket_fd) { int ret = listen(socket_fd, 16); if (ret == -1) { panic(\u0026#34;listen() error: %s\\n\u0026#34;, strerror(errno)); } } int Accept(int socket_fd, struct sockaddr_in *addr, socklen_t *addr_len) { int fd = accept(socket_fd, (struct sockaddr *)addr, addr_len); if (fd == -1) { panic(\u0026#34;accept() error: %s\\n\u0026#34;, strerror(errno)); } int ret = set_nonblocking(fd); if (ret == -1) { panic(\u0026#34;set_nonblocking error\\n\u0026#34;); } return fd; } void Connect(int socket_fd, struct sockaddr_in *addr) { int ret = connect(socket_fd, (struct sockaddr *)(addr), sizeof(*addr)); if (ret == -1) { panic(\u0026#34;connect() error: %s\\n\u0026#34;, strerror(errno)); } } 客户端 # 客户端读取一行用户的输入，将数据传给服务器，服务器将所有的字母大写后再传给客户端。\nvoid run_client(const char *remote_address, int remote_port) { int client_fd = Socket(); struct sockaddr_in *server_addr = create_sockaddr_in(remote_address, remote_port); Connect(client_fd, server_addr); for (;;) { char *input = read_line(); size_t input_len = strlen(input); char buf[input_len + 1]; if (input_len == 0) { goto out; } ssize_t sent_bytes = send(client_fd, input, input_len, 0); if (sent_bytes == -1) { panic(\u0026#34;send() error: %s\\n\u0026#34;, strerror(errno)); } ssize_t recv_bytes = recv(client_fd, buf, input_len, 0); if (recv_bytes == -1) { panic(\u0026#34;recv() error: %s\\n\u0026#34;, strerror(errno)); } printf(\u0026#34;%s\\n\u0026#34;, buf); out: free(input); } } 为什么多路复用需要搭配非阻塞 # On Linux, select() may report a socket file descriptor as \u0026#34;ready for reading\u0026#34;, while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has the wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block. 参考链接： select\n普通版本 # #include \u0026lt;ctype.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;stdarg.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; int used_for_cleanup_fd = -1; void panic(const char *format, ...) { va_list args; va_start(args, format); vfprintf(stderr, format, args); va_end(args); exit(EXIT_FAILURE); } void cleanup() { if (used_for_cleanup_fd != -1) close(used_for_cleanup_fd); } void run_client(const char *remote_address, int remote_port) { int ret; char buf[512]; int client_fd = socket(AF_INET, SOCK_STREAM, 0); if (client_fd == -1) { panic(\u0026#34;socket() error: %s\\n\u0026#34;, strerror(errno)); } atexit(cleanup); used_for_cleanup_fd = client_fd; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = inet_addr(remote_address); server_addr.sin_port = htons(remote_port); ret = connect(client_fd, (struct sockaddr *)(\u0026amp;server_addr), sizeof(server_addr)); if (ret == -1) { panic(\u0026#34;connect() error: %s\\n\u0026#34;, strerror(errno)); } for (;;) { fgets(buf, sizeof(buf), stdin); buf[strcspn(buf, \u0026#34;\\n\u0026#34;)] = \u0026#39;\\0\u0026#39;; size_t buf_len = strlen(buf); if (buf_len == 0) { continue; } if (buf_len \u0026gt;= sizeof(buf) - 1) { panic(\u0026#34;input too long\\n\u0026#34;); } ssize_t sent_bytes = send(client_fd, buf, buf_len, 0); if (sent_bytes == -1) { panic(\u0026#34;send() error: %s\\n\u0026#34;, strerror(errno)); } ssize_t recv_bytes = recv(client_fd, buf, sizeof(buf), 0); if (recv_bytes == -1) { panic(\u0026#34;recv() error: %s\\n\u0026#34;, strerror(errno)); } printf(\u0026#34;%s\\n\u0026#34;, buf); } } void run_server(const char *address, int port) { int ret; char buf[512]; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = inet_addr(address); server_addr.sin_port = htons(port); int server_fd = socket(AF_INET, SOCK_STREAM, 0); if (server_fd == -1) { panic(\u0026#34;socket() error: %s\\n\u0026#34;, strerror(errno)); } atexit(cleanup); used_for_cleanup_fd = server_fd; ret = bind(server_fd, (struct sockaddr *)(\u0026amp;server_addr), sizeof(server_addr)); if (ret == -1) { panic(\u0026#34;bind() error: %s\\n\u0026#34;, strerror(errno)); } ret = listen(server_fd, 16); if (ret == -1) { panic(\u0026#34;listen() error: %s\\n\u0026#34;, strerror(errno)); } struct sockaddr_in client_addr; socklen_t client_addr_len = sizeof(client_addr); for (;;) { int client_fd = accept(server_fd, (struct sockaddr *)(\u0026amp;client_addr), \u0026amp;client_addr_len); if (client_fd == -1) { panic(\u0026#34;accept() error: %s\\n\u0026#34;, strerror(errno)); } for (;;) { ssize_t read_bytes = recv(client_fd, buf, sizeof(buf), 0); if (read_bytes == -1) { panic(\u0026#34;recv() error: %s\\n\u0026#34;, strerror(errno)); } if (read_bytes == 0) { close(client_fd); break; } for (ssize_t i = 0; i \u0026lt; read_bytes; i++) { buf[i] = (char)toupper((unsigned char)buf[i]); } ssize_t sent_bytes = send(client_fd, buf, read_bytes, 0); if (sent_bytes == -1) { panic(\u0026#34;send() error: %s\\n\u0026#34;, strerror(errno)); } } } } int main(int argc, char *argv[]) { if (argc \u0026lt; 4) { panic(\u0026#34;usage: \u0026lt;mode\u0026gt; \u0026lt;IP\u0026gt; \u0026lt;port\u0026gt;\\n\u0026#34;); } char *mode = argv[1]; char *address = argv[2]; int port = atoi(argv[3]); if (strncmp(mode, \u0026#34;client\u0026#34;, 6) == 0) run_client(address, port); if (strncmp(mode, \u0026#34;server\u0026#34;, 6) == 0) run_server(address, port); return 0; } select 版本 # #include \u0026lt;ctype.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;stdarg.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;sys/select.h\u0026gt; #define MAX(a, b) a \u0026lt; b ? b : a int used_for_cleanup_fd = -1; void panic(const char *format, ...) { va_list args; va_start(args, format); vfprintf(stderr, format, args); va_end(args); exit(EXIT_FAILURE); } void cleanup() { if (used_for_cleanup_fd != -1) close(used_for_cleanup_fd); } void run_client(const char *remote_address, int remote_port) { int ret; char buf[512]; int client_fd = socket(AF_INET, SOCK_STREAM, 0); if (client_fd == -1) { panic(\u0026#34;socket() error: %s\\n\u0026#34;, strerror(errno)); } atexit(cleanup); used_for_cleanup_fd = client_fd; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = inet_addr(remote_address); server_addr.sin_port = htons(remote_port); ret = connect(client_fd, (struct sockaddr *)(\u0026amp;server_addr), sizeof(server_addr)); if (ret == -1) { panic(\u0026#34;connect() error: %s\\n\u0026#34;, strerror(errno)); } for (;;) { fgets(buf, sizeof(buf), stdin); buf[strcspn(buf, \u0026#34;\\n\u0026#34;)] = \u0026#39;\\0\u0026#39;; size_t buf_len = strlen(buf); if (buf_len == 0) { continue; } if (buf_len \u0026gt;= sizeof(buf) - 1) { panic(\u0026#34;input too long\\n\u0026#34;); } ssize_t sent_bytes = send(client_fd, buf, buf_len, 0); if (sent_bytes == -1) { panic(\u0026#34;send() error: %s\\n\u0026#34;, strerror(errno)); } ssize_t recv_bytes = recv(client_fd, buf, sizeof(buf), 0); if (recv_bytes == -1) { panic(\u0026#34;recv() error: %s\\n\u0026#34;, strerror(errno)); } printf(\u0026#34;%s\\n\u0026#34;, buf); } } void run_server(const char *address, int port) { int ret; char buf[512]; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = inet_addr(address); server_addr.sin_port = htons(port); int server_fd = socket(AF_INET, SOCK_STREAM, 0); if (server_fd == -1) { panic(\u0026#34;socket() error: %s\\n\u0026#34;, strerror(errno)); } atexit(cleanup); used_for_cleanup_fd = server_fd; ret = bind(server_fd, (struct sockaddr *)(\u0026amp;server_addr), sizeof(server_addr)); if (ret == -1) { panic(\u0026#34;bind() error: %s\\n\u0026#34;, strerror(errno)); } ret = listen(server_fd, 16); if (ret == -1) { panic(\u0026#34;listen() error: %s\\n\u0026#34;, strerror(errno)); } struct sockaddr_in client_addr; socklen_t client_addr_len = sizeof(client_addr); fd_set fds; FD_ZERO(\u0026amp;fds); FD_SET(server_fd, \u0026amp;fds); int max_fd = server_fd; for (;;) { fd_set rfds = fds; int retval = select(max_fd + 1, \u0026amp;rfds, NULL, NULL, NULL); if (retval == -1) { panic(\u0026#34;select() error: %s\\n\u0026#34;, strerror(errno)); } if (FD_ISSET(server_fd, \u0026amp;rfds)) { // new connection int client_fd = accept(server_fd, (struct sockaddr *)(\u0026amp;client_addr), \u0026amp;client_addr_len); if (client_fd == -1) { panic(\u0026#34;accept() error: %s\\n\u0026#34;, strerror(errno)); } FD_SET(client_fd, \u0026amp;fds); max_fd = MAX(max_fd, client_fd); if (retval == 1) continue; } for (int fd = server_fd + 1; fd \u0026lt;= max_fd; fd++) { if (FD_ISSET(fd, \u0026amp;rfds)) { ssize_t read_bytes = recv(fd, buf, sizeof(buf), 0); if (read_bytes == -1) { panic(\u0026#34;recv() error: %s\\n\u0026#34;, strerror(errno)); } if (read_bytes == 0) { FD_CLR(fd, \u0026amp;fds); close(fd); break; } for (ssize_t i = 0; i \u0026lt; read_bytes; i++) { buf[i] = (char)toupper((unsigned char)buf[i]); } ssize_t sent_bytes = send(fd, buf, read_bytes, 0); if (sent_bytes == -1) { panic(\u0026#34;send() error: %s\\n\u0026#34;, strerror(errno)); } } } } } int main(int argc, char *argv[]) { if (argc \u0026lt; 4) { panic(\u0026#34;usage: \u0026lt;mode\u0026gt; \u0026lt;IP\u0026gt; \u0026lt;port\u0026gt;\\n\u0026#34;); } char *mode = argv[1]; char *address = argv[2]; int port = atoi(argv[3]); if (strncmp(mode, \u0026#34;client\u0026#34;, 6) == 0) run_client(address, port); if (strncmp(mode, \u0026#34;server\u0026#34;, 6) == 0) run_server(address, port); return 0; } "},{"id":10,"href":"/docs/Java/Cleaner/","title":"Cleaner 类","section":"Java","content":" 参考文献 # https://openjdk.org/jeps/421 https://inside.java/2022/05/25/clean-cleaner/ https://docs.oracle.com/javase/9/docs/api/java/lang/ref/Cleaner.html "},{"id":11,"href":"/docs/reports/reports/","title":"周报","section":"Docs","content":" 周报 # 2023-03-02 # 本周工作 # 调研了多光谱无人机（DJI Mavic 3M），感觉还是廷贵的，要 2.8 万。 这周要周报，了解了一下深度学习编译器，感觉还是很神奇的。 科研目前打算研究轻量化神经网络，学习了 MobileNet，感叹之强大。 学了学 Java 虚拟机的类加载，由于各种历史原因，还是很复杂的。 下周计划 # 学完 MobileNet，然后学习一下 Xception 和 SqueezeNet。 继续学习 Java 虚拟机类加载机制。 不能落下刷算法，继续刷力扣，主要研究动态规划。 "}]