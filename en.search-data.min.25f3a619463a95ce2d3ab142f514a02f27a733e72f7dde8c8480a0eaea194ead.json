[{"id":0,"href":"/docs/distributed-system/basic-theory-of-distributed-system/","title":"分布式系统基本理论","section":"Distributed System","content":"分布式系统基本理论 #  "},{"id":1,"href":"/docs/distributed-system/consensus-algorithm/","title":"分布式共识算法","section":"Distributed System","content":"分布式一致性算法 #  Raft 算法是 Stanford University 的 Diego Ongaro 的博士论文，是分布式一致性算法的一种，相比于 Paxos 算法更易于理解，使用该算法的项目有：TiKV, etcd, CockroachDB 等\n官方地址：https://raft.github.io/\n 时间 (Terms) #  分布式系统中节点的时间是不可靠的，如不采取特殊硬件，节点与节点之间时间是存在一定的误差。如果采用时间作为判断操作的先后顺序，即使是很细微的误差仍然会导致误判。因此 Raft 把时间划分为不同的 terms（任期），term 类似于逻辑时钟，可以用于判断事件发生的先后顺序。\n如果当前节点接收到较大 term 节点的请求，则说明当前节点已经落后于请求的节点，需要转为 Follower 状态，并把当前 term 更新为较大的 term。\n 角色 (states) #  Raft 中的节点始终处于 Follower | Candidate | Leader 三种状态之一，不同状态之间根据不同的触发条件进行相互转换。\n  跟随者 (Follower) ：当节点启动后，此时节点为 Follower 状态，在一定时间内（300~500ms）如果没有收到来自 Leader 的心跳包，此时称为选举超时（election timeout），当前节点则会转为 Candidate 状态 候选者 (Candidate) ：当转为 Candidate 状态后，Candidate 节点会先投自己，然后向集群中的其它节点（不包含自己）发送投票请求，请求获取投票。如果集群中超过一半的节点同意该投票请求，该 Candidate 节点就会转为Leader 状态 领导者 (Leader) ：当节点转为 Leader 状态后，会定时给集群中的所有节点发送心跳包，维护其 Leader 状态。Leader 节点会处理来自 clients 的所有请求，当集群中处于其它状态的节点接收到来自 clients 的请求后，会转发给 Leader 进行处理  保证一致性 #  Raft 把保证一致性问题分为三个相对独立的子问题：\n 选举 (Leader Election) ：当集群中的 Leader 宕机后，保证选举出一个新的 Leader 日志复制 (Log replication) ：当 Leader 接收到 clients 的日志请求后，需保证复制到集群中的所有节点 安全性 (Safety) ：需要采用一些额外的规则来保证运行正确  全局状态 #    一些需要保存的重要状态，分为持久化保存，非持久化保存和仅 Leader 节点非持久化保存\n 持久化保存 #   currentTerm : 当前节点的 term，初始化为 0 ，每开启一轮新的选举 currentTerm++ voteFor : 当前 term 已投票的节点的 ID。当为 None 时说明当前 term 无投票，可以对验证过的 Candidate 节点进行投票 log[] : 用于当前节点保存 Log Entry，索引下标从 1 开始  非持久化保存 #  当一条日志被提交（commit）后，意味着该条日志已经复制到集群中的大部分节点中，此时就可以把应用（apply）到状态机中，永远满足：lastApplied \u0026lt;= commitIndex。\n// apply 的流程 for (int i = lastApplied; i \u0026lt;= commitIndex; i++) { apply(log[i]); // apply to state machine  lastApplied++; }  commitIndex : 已经提交的 Log Entry 中最大的 Index lastApplied : 已经 apply 到状态机中的 Log Entry 中的最大 Index  仅 Leader 节点 #  利用数组保存集群节点的状态，len(nextIndex[]) = len(matchIndex[]) = len(集群节点)，下标保存就是对应节点信息，nextIndex[0] 和 matchIndex[0] 表示集群中的第 0 个节点状态，nextIndex[1] 和 matchIndex[1] 表示集群中的第 1 个节点，以此类推。\n如果 Leader 节点通过 AppendEntries RPC 复制 Log Entry 到 Follower 节点成功后，nextIndex[ID] += len(entries); matchIndex[ID] += len(entries);\n当复制完成后，可以对 matchIndex[] 进行排序，中位数就是已经复制到大多数节点的 Log Entry 的 index，即可以用于更新 commitIndex。\n当 matchIndex = {1, 5, 5, 7, 9}; 中位数为 5，集群中 matchIndex[i] \u0026lt;= 5 的节点个数为 3 个 len(matchIndex)/2 \u0026lt; 3，即集群中的大多数节点的 matchIndex 都大于等于 5 每次选举成功后，Leader 节点需要重新初始化下边的状态\n  nextIndex[] : 下次给对应 server 发送日志中的第一个 Log Entry 的 index，初始为 Leader 节点最后一个 Log Entry 的 index + 1。当发送的日志与 Follower 节点的日志冲突时，则会回退 nextIndex[id]--\n  matchIndex[] : 对应的 server 已经复制完成的 Log Entry 的 index，初始为 0\n  选举 #  当节点启动后，节点为 Follower 状态，如果在一定时间内（300~500ms），没有接收到集群中 Leader 发送的有效的心跳包，此时称为 election timeout。\n当节点选举超时后，则会转为 Candidate 状态。首先会先投自己一票，然后发起选举请求，并发的向集群中的其它节点发起 RequestVote RPC 请求，请求获取集群中其它节点的投票，如果 获取到的投票数 \u0026gt; len(集群节点) / 2 则代表集群一半以上的节点同意了该投票请求，该节点即可变为集群中的 Leader，处理来自 clients 的所有请求。\n选举超时说明集群中无 term 大于当前节点并处于 Leader 状态的节点，但集群中可能存在 term 小于当前节点，但处于 Leader 状态的节点。当节点收到 term 较大节点发送的 RPC 请求后，则说明该节点已经滞后于较大 term 的节点，此时较小 term 的节点会转为 Follower 状态并更新当前 term 为较大节点的 term。\nRequestVote RPC 请求 #    请求投票 RPC 请求：该请求会用于 Candidate 节点获取 Follower 节点的投票\n RequestVote Arguments #  lastLogIndex 和 lastLogTerm 用于保证当选 Leader 的节点包含集群中已被提交的所有 Log Entry。\n term : Candidate 节点的 term，用于后续投票者判断 Candidate 节点是否滞后。  Candidate's term \u0026gt;= currentTerm : 当前节点在该 term 任期如果没有投过其它节点，把 voteFor 设为 Candidate 节点的 ID，代表当前轮已经投过了，在该 term 任期中不会再投其它节点 Candidate's term \u0026lt; currentTerm : Candidate 节点滞后与当前节点，拒绝其投票请求   candidateId : Candidate 节点的 ID 编号，用于投票者设置该 term 任期内投票的节点，即设置 voteFor = candidateID lastLogIndex : 当前 Candidate 节点最后一条 Log Entry 的 index lastLogTerm : 当前 Candidate 节点最后一条 Log Entry 的 term  RequestVote Response #   term : 当前 Follower 节点的 term，用于 Candidate 节点判断自己是否滞后与该 Follower 节点。如果 Follower's term \u0026gt; currentTerm，则该 Candidate 节点转为 Follower 状态 voteGranted : Follower 节点同意该 Candidate 节点的投票请求  日志复制 #    日志状态机 (State Machine) : 采用多个节点保存同一份 Log Entry，即使一个节点宕机，其它节点仍然可以提供服务\n 当接收到 clients 的请求后，整个流程如下：\n 当 Leader 节点接收到来自 clients 的需要 apply 到状态机中 Log Entry 请求后，则把 Log Entry 添加到自身的 Log 中，随后通过 AppendEntries RPC 并发的向集群中的其他节点发送请求 当 Follower 节点添加 Log Entry 到自身的 Log 中后，会返回 AppendEntries Response 响应，Leader 节点通过 success 是否为真来判断该节点是否成功保存到该 Follower 节点中 如果一半以上的节点保存该 Log Entry 到自身 Log 中，Leader 节点就会更新本身的 committedIndex 为该 Log Entry 的 Index，下一次发送 AppendEntry RPC 请求时，Follower 节点会根据 leaderCommit 来提交该 Log Entry 和之前未提交的 Log Entry，随后 apply 到状态机中，并返回执行结果给 clients 如果某个 Follower 节点宕机了，或者由于网络丢包导致 RPC 请求没有送到节点中，则 Leader 会一直重试发送该请求  Raft 保证一旦一条 Log Entry 被 committed 后，该条 Log Entry 会被永久存储。即使节点宕机恢复后，恢复后该条日志仍然存在。而来自 clients 请求中的 Log Entry 不需要立即持久化存储，因为 clients 的每条请求会设置一定的超时时间，如果节点宕机导致该 Log Entry 丢失，该条请求就会超时，clients 节点只需重新发送该请求即可。\nRaft 使用 AppendEntries RPC 请求作为日志复制的载体，只有 Leader 节点才会发送 AppendEntries RPC 请求。通常设置 30ms 发送一次 AppendEntries RPC 请求。如发送频率过高，则会导致发送 RPC 请求消耗过高；如发送频率过低，会导致节点之间信息同步过慢。\n根据 entries[] 是否为空，分为两种用途的 RPC 请求：\n 当 entries[] 为空时：该 RPC 请求作为 Leader 发送的 heartbeat，维护其 Leader 状态 当 entries[] 不为空时：该请求会携带 Log Entry，用于 Follower 节点保存 Log 到状态机中。在两次 RPC 请求过程中可能会有多条 Log 请求，因此可以放到一次 RPC 请求中发送。携带 Log 的 RPC 请求也可以作为 heartbeat 使用  AppendEntries RPC 请求 #    日志添加 RPC 请求：用于 Leader 节点发送 heartbeat 或日志复制请求\n AppendEntries Request #  prevLogIndex 和 prevLogTerm 是 Follower 节点用来检查自身 Log Entry 是否和 Leader 节点的 Log Entry 一致。如果当前 Follower 节点保存的最后一条 Log Entry 的 index 和 term 与 prevLogIndex 和 prevLogTerm 一致，则说明没有冲突，直接把新的 Log Entry 添加到 log[] 尾部即可。如果冲突，则直接拒绝该条 RPC 请求，设置 sucess 为 false。随后 Leader 节点会进行回退，把 nextIndex[ID]--。下次就会发送前一条 Log Entry，直到 Leader 节点和 Follower 节点 Log Entry 不冲突为止。\n如果 Follower 节点保存的有 index 为 prevLogIndex 的 Log Entry，并且保存有大于 index 的节点，则强制删除 index 大于 prevLogIndex 后的所有 Log Entry。\n是本次 RPC 请求中的的 entries[0] 位置的 Log Entry 的前一个位于 log[] 中的 LogEntry 的 index 和 term。\n term : 当前节点的 term，用于 Follower 节点判断是否是过期的 Leader 节点。  Leader's term \u0026gt;= currentTerm : 当前节点在该 term 任期如果没有投过其它节点，把 voteFor 设为 Candidate 节点的 ID，代表当前轮已经投过了，在该 term 任期中不会再投其它节点 Leader's term \u0026lt; currentTerm : Candidate 节点滞后与当前节点，拒绝其投票请求   leaderId : Candidate 节点可以保存 leaderId，当 clients 发送请求给 Follower 节点后，Follower 节点可以通过把 clients 发到为 leaderId 的节点 prevLogIndex : prevLogIndex = nextIndex[ID] - 1 prevLogTerm : prevLogTerm = log[prevLogIndex].term entries[] : entries = log[nextIndex[ID]:len(log)-1]，用于保存此次需要复制到 Follower 节点的 Log Entry，如果 len(entries) == 0 ，则用于 heartbeat leaderCommit : Leader 节点已提交 Log Entry 的最后一条的 Index。Follower 节点根据此索引来确定已经被提交的 Log Entry，进行提交  AppendEntries Response #   term : Follower 节点当前的 term success : 当前 Follower 节点与 Leader 节点保存的 Log Entry 是否冲突  "},{"id":2,"href":"/docs/distributed-system/distributed-transaction/","title":"分布式事务处理","section":"Distributed System","content":"分布式事务处理 #  "},{"id":3,"href":"/docs/program-language/concurrent-programming-in-java/","title":"Java 并发编程","section":"Program Language","content":"Java 并发编程 #  "}]